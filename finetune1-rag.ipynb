{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:28:36.618695Z","iopub.execute_input":"2025-03-09T13:28:36.619074Z","iopub.status.idle":"2025-03-09T13:28:36.946511Z","shell.execute_reply.started":"2025-03-09T13:28:36.619044Z","shell.execute_reply":"2025-03-09T13:28:36.945779Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ntorch.cuda.memory_summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:28:36.947496Z","iopub.execute_input":"2025-03-09T13:28:36.947926Z","iopub.status.idle":"2025-03-09T13:28:40.148503Z","shell.execute_reply.started":"2025-03-09T13:28:36.947902Z","shell.execute_reply":"2025-03-09T13:28:40.147733Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:28:40.149882Z","iopub.execute_input":"2025-03-09T13:28:40.150226Z","iopub.status.idle":"2025-03-09T13:28:40.384729Z","shell.execute_reply.started":"2025-03-09T13:28:40.150202Z","shell.execute_reply":"2025-03-09T13:28:40.383674Z"}},"outputs":[{"name":"stdout","text":"Sun Mar  9 13:28:40 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   40C    P8             10W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   43C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n# ‚úÖ Prevent CUDA memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\ndef fine_tune_flan_t5():\n    dataset = load_dataset(\"squad_v2\")\n\n    # ‚úÖ Remove examples with empty answers\n    def filter_empty_answers(example):\n        return bool(example[\"answers\"][\"text\"])  \n\n    dataset[\"train\"] = dataset[\"train\"].filter(filter_empty_answers)\n    dataset[\"validation\"] = dataset[\"validation\"].filter(filter_empty_answers)\n\n    print(f\"Train set size after filtering: {len(dataset['train'])}\")\n    print(f\"Validation set size after filtering: {len(dataset['validation'])}\")\n\n    # ‚úÖ Use a subset for memory efficiency\n    dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(2000))  \n    dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(500))\n\n    print(\"\\nExample data after filtering:\", dataset[\"train\"][0])\n\n    test_inputs = [\n        (dataset[\"train\"][0][\"context\"], dataset[\"train\"][0][\"question\"]),\n        (dataset[\"train\"][1][\"context\"], dataset[\"train\"][1][\"question\"])\n    ]\n\n    # ‚úÖ Use FLAN-T5 Large (Memory Optimized)\n    model_name = \"google/flan-t5-small\"  \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",  # ‚úÖ Automatically selects GPU\n        torch_dtype=torch.bfloat16,  # ‚úÖ Use BF16 instead of FP16 (Better for T5 models)\n    )\n\n    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Ensure padding token is set\n\n    def preprocess_function(examples):\n        \"\"\" ‚úÖ Format input as structured prompt for FLAN-T5 \"\"\"\n        inputs = [\n            f\"Answer the question based on the provided context:\\n\\nContext: {c}\\n\\nQuestion: {q}\\n\\nAnswer:\"\n            for c, q in zip(examples[\"context\"], examples[\"question\"])\n        ]\n\n        # ‚úÖ Ensure answers are non-empty, otherwise use \"No Answer\"\n        outputs = [a[\"text\"][0] if len(a[\"text\"]) > 0 else \"No Answer\" for a in examples[\"answers\"]]\n\n        tokenized_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n        tokenized_outputs = tokenizer(outputs, padding=\"max_length\", truncation=True, max_length=128)\n\n        tokenized_inputs[\"labels\"] = tokenized_outputs[\"input_ids\"]\n\n        return tokenized_inputs\n\n    # ‚úÖ Apply preprocessing\n    tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n\n    # ‚úÖ Memory-Optimized Training Arguments\n    training_args = TrainingArguments(\n        output_dir=\"./flan_t5_results\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  \n        learning_rate=3e-5,\n        per_device_train_batch_size=2,  # ‚úÖ Lower batch size to avoid OOM\n        per_device_eval_batch_size=2,\n        num_train_epochs=5,  \n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        log_level=\"info\",\n        report_to=\"none\",\n        gradient_accumulation_steps=4,  # ‚úÖ Helps with batch size reduction\n        gradient_checkpointing=True,  # ‚úÖ Reduce memory by recomputing activations\n        bf16=True,  # ‚úÖ BF16 is better for T5 models\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"validation\"],\n        tokenizer=tokenizer,\n    )\n\n    print(\"\\nüöÄ Starting Training...\")\n    with torch.autocast(\"cuda\", dtype=torch.bfloat16):  # ‚úÖ Enable mixed precision training\n        trainer.train()\n\n    eval_results = trainer.evaluate()\n    print(f\"\\nüî• Final Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n\n    trainer.save_model(\"./fine_tuned_flan_t5\")\n    print(\"\\n‚úÖ Fine-tuned model saved!\")\n\n    return model, tokenizer, test_inputs\n\ndef test_flan_t5_model(model, tokenizer, test_inputs):\n    print(\"\\nüîç Running test queries from the training set...\\n\")\n\n    for context, query in test_inputs:\n        prompt = f\"Answer the question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n        output = model.generate(\n            **inputs, \n            max_new_tokens=256,  \n            temperature=0.7,  \n            num_beams=3,  \n            repetition_penalty=1.2,  \n            do_sample=True\n        )\n        print(f\"‚ùì Question: {query}\")\n        print(f\"üìñ Context: {context[:200]}...\")  \n        print(f\"üí° Answer: {tokenizer.decode(output[0], skip_special_tokens=True)}\\n\")\n\nif __name__ == \"__main__\":\n    model, tokenizer, test_inputs = fine_tune_flan_t5()\n    test_flan_t5_model(model, tokenizer, test_inputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:28:40.386088Z","iopub.execute_input":"2025-03-09T13:28:40.386375Z","iopub.status.idle":"2025-03-09T13:47:58.308506Z","shell.execute_reply.started":"2025-03-09T13:28:40.386352Z","shell.execute_reply":"2025-03-09T13:47:58.307805Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c7fa7f77e14766874227e85e277896"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb3af903622a4d009c8c82a560b61eef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc7757cf54d2483ba3826de63c1aebfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8129e1e571403ebd0d39cd56b7b7c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c1c58c554b14073a732f4207d856ea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e29a0245a0b427899a518470347a968"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af70b1215f04a2188b065555a29861a"}},"metadata":{}},{"name":"stdout","text":"Train set size after filtering: 86821\nValidation set size after filtering: 5928\n\nExample data after filtering: {'id': '572817584b864d1900164463', 'title': 'London', 'context': 'Outward urban expansion is now prevented by the Metropolitan Green Belt, although the built-up area extends beyond the boundary in places, resulting in a separately defined Greater London Urban Area. Beyond this is the vast London commuter belt. Greater London is split for some purposes into Inner London and Outer London. The city is split by the River Thames into North and South, with an informal central London area in its interior. The coordinates of the nominal centre of London, traditionally considered to be the original Eleanor Cross at Charing Cross near the junction of Trafalgar Square and Whitehall, are approximately 51¬∞30‚Ä≤26‚Ä≥N 00¬∞07‚Ä≤39‚Ä≥W\\ufeff / \\ufeff51.50722¬∞N 0.12750¬∞W\\ufeff / 51.50722; -0.12750.', 'question': 'Greater London is divided into what two groups of boroughs?', 'answers': {'text': ['Inner London and Outer London'], 'answer_start': [293]}}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"718096bd0a8d45c2a2df15002de38c7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf54bb4a418e4e4bb8c908a0dec34248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac30508213484395a26e6691eb67721f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c1d7e0dce34ee9a0d125a6fba005e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca604dbf157142238b9a5905c8048bfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a04d7065bda4356ae0e4840675cfd2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d93c285d534c77b10c5c440929c0b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5534108d5a6453885a056d6074e8d7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc62d00620e64236bed046c9bbf41a60"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-4-a900fca5b833>:84: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nYou have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\nUsing auto half precision backend\n***** Running training *****\n  Num examples = 2,000\n  Num Epochs = 5\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 1,250\n  Number of trainable parameters = 76,961,152\n","output_type":"stream"},{"name":"stdout","text":"\nüöÄ Starting Training...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 18:31, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.166200</td>\n      <td>0.042428</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.045300</td>\n      <td>0.027630</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.038000</td>\n      <td>0.026731</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.045200</td>\n      <td>0.026593</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.044400</td>\n      <td>0.026514</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 2\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 2\n\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 2\n\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 2\n\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 2\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n\n***** Running Evaluation *****\n  Num examples = 500\n  Batch size = 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:13]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to ./fine_tuned_flan_t5\nConfiguration saved in ./fine_tuned_flan_t5/config.json\nConfiguration saved in ./fine_tuned_flan_t5/generation_config.json\n","output_type":"stream"},{"name":"stdout","text":"\nüî• Final Evaluation Loss: 0.0265\n","output_type":"stream"},{"name":"stderr","text":"Model weights saved in ./fine_tuned_flan_t5/model.safetensors\ntokenizer config file saved in ./fine_tuned_flan_t5/tokenizer_config.json\nSpecial tokens file saved in ./fine_tuned_flan_t5/special_tokens_map.json\nCopy vocab file to ./fine_tuned_flan_t5/spiece.model\n","output_type":"stream"},{"name":"stdout","text":"\n‚úÖ Fine-tuned model saved!\n\nüîç Running test queries from the training set...\n\n‚ùì Question: Greater London is divided into what two groups of boroughs?\nüìñ Context: Outward urban expansion is now prevented by the Metropolitan Green Belt, although the built-up area extends beyond the boundary in places, resulting in a separately defined Greater London Urban Area. ...\nüí° Answer: Inner London and Outer London\n\n‚ùì Question: Where is the Opera House located?\nüìñ Context: The German Renaissance has bequeathed the city some noteworthy buildings (especially the current Chambre de commerce et d'industrie, former town hall, on Place Gutenberg), as did the French Baroque an...\nüí° Answer: Place Broglie\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}