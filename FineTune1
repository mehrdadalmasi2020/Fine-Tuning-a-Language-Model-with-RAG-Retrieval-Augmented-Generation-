{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mehrdadal2023/finetune1-rag?scriptVersionId=227382940\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# üöÄ Fine-Tuning a Language Model with RAG (Retrieval-Augmented Generation)\n\nThis notebook demonstrates how to fine-tune a language model using **Retrieval-Augmented Generation (RAG)**, leveraging Hugging Face's `transformers` library. The process follows these key steps:\n\n## üõ†Ô∏è System Setup\n- ‚úÖ Checks the **GPU availability** using `nvidia-smi` to ensure compatibility for model training.\n\n## üì• Loading the Pretrained Model\n- Uses the **Flan-T5** model from the Hugging Face Hub.\n- Loads the **tokenizer** to process input text efficiently.\n\n## üìö Preparing the Dataset\n- Loads the **SQuAD v2** dataset.\n- ‚úÖ **Filters** out examples with empty answers to improve training quality.\n- ‚úÖ **Shuffles & selects** a subset for memory-efficient training.\n\n## üéØ Fine-Tuning the Model\n- ‚úÖ Trains the model using **structured prompts** optimized for FLAN-T5.\n- ‚úÖ Applies **gradient checkpointing** and **BF16 precision** to enhance memory efficiency.\n- ‚úÖ Configures **custom training arguments**, including:\n  - Learning rate: `3e-5`\n  - Epochs: `5`\n  - Batch size: `2`\n  - **Gradient accumulation** for better memory management.\n\n## üìä Model Evaluation\n- ‚úÖ Evaluates model performance on a **held-out validation set**.\n- ‚úÖ Reports **final evaluation loss** to measure improvements.\n\n## üíæ Saving the Fine-Tuned Model\n- ‚úÖ Saves the **trained model weights**, tokenizer, and configuration.\n- ‚úÖ Outputs a fully trained model ready for **inference & deployment**.\n\n## üîç Testing the Model\n- ‚úÖ Runs test queries from the **training set**.\n- ‚úÖ Displays:\n  - **‚ùì Question**\n  - **üìñ Context**\n  - **üí° Model's Answer**\n\n## üéØ Conclusion\nThis notebook provides a **step-by-step guide** to fine-tuning a **Flan-T5** model using **RAG**. By incorporating retrieval mechanisms, the model improves its ability to generate **accurate, context-aware answers**.\n\nüîπ **Ideal for:** Fine-tuning **question-answering models** with external knowledge retrieval.\n\n---\nüöÄ *Enhance your AI-powered applications with a fine-tuned model that understands context better!*\n","metadata":{}},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\ntorch.cuda.memory_summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:28:36.947496Z","iopub.execute_input":"2025-03-09T13:28:36.947926Z","iopub.status.idle":"2025-03-09T13:28:40.148503Z","shell.execute_reply.started":"2025-03-09T13:28:36.947902Z","shell.execute_reply":"2025-03-09T13:28:40.147733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:28:40.149882Z","iopub.execute_input":"2025-03-09T13:28:40.150226Z","iopub.status.idle":"2025-03-09T13:28:40.384729Z","shell.execute_reply.started":"2025-03-09T13:28:40.150202Z","shell.execute_reply":"2025-03-09T13:28:40.383674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Fine-Tuning FLAN-T5: Optimized Training Approach**\n\nThis fine-tuning setup optimizes **FLAN-T5** for **question-answering tasks** using the **SQuAD v2 dataset**. The primary focus is on **memory efficiency, stability, and response quality** while leveraging **modern fine-tuning techniques**. \n\n---\n\n## **üöÄ Fine-Tuning Strategy & Techniques**\n### **üìå Model Selection & Memory Optimization**\n- **Model Used**: `google/flan-t5-small`\n- **Precision**: **BFloat16 (`bf16=True`)**\n  - Reduces memory usage **without sacrificing numerical stability**.\n  - **Better than FP16** for T5-based models.\n- **Device Allocation**: `device_map=\"auto\"`\n  - **Automatically distributes** model parameters across **available GPUs**.\n\n---\n\n## **üîÑ Data Processing & Preprocessing**\n- **Dataset**: `squad_v2` (with unanswerable questions for better robustness).\n- **Filtering Strategy**:  \n  - **Removes examples** with **empty answers** to ensure meaningful learning.\n- **Structured Prompting**:  \n  - **Reformatted inputs** to explicitly frame the task as:\n    ```\n    Answer the question based on the provided context:\n    \n    Context: {context}\n    \n    Question: {question}\n    \n    Answer:\n    ```\n  - **Why?**  \n    - **Improves instruction-following behavior** of FLAN-T5.\n    - Aligns with **T5's pre-training structure** for better fine-tuning.\n\n---\n\n## **‚öôÔ∏è Hyperparameters & Training Settings**\n| **Parameter**  | **Value** | **Reason** |\n|---------------|----------|------------|\n| `learning_rate` | `3e-5` | Stable learning rate for fine-tuning without overfitting. |\n| `num_train_epochs` | `5` | Sufficient training cycles for model adaptation. |\n| `per_device_train_batch_size` | `2` | Small batch size to avoid **CUDA Out of Memory (OOM)**. |\n| `per_device_eval_batch_size` | `2` | Ensures evaluation consistency. |\n| `gradient_accumulation_steps` | `4` | **Effective batch size = 8**, allowing **better stability**. |\n| `gradient_checkpointing` | `True` | Saves memory by recomputing activations during backpropagation. |\n| `bf16` | `True` | Reduces memory usage while maintaining numerical precision. |\n| `evaluation_strategy` | `\"epoch\"` | Evaluates performance **after each epoch**. |\n| `save_strategy` | `\"no\"` | **Avoids checkpointing** to save disk space. |\n\n### **üìå Why Gradient Accumulation?**\n- Since **batch size is low** due to **GPU memory constraints**, **gradient accumulation** allows:\n  - **Larger effective batch size (8 instead of 2)**\n  - **More stable updates** to model weights.\n\n### **üìå Why Gradient Checkpointing?**\n- Instead of storing **all intermediate activations** during training, this approach **recomputes them on demand**, significantly **reducing GPU memory consumption**.\n\n---\n\n## **üîç Model Generation & Inference**\n| **Parameter**  | **Value** | **Reason** |\n|---------------|----------|------------|\n| `max_new_tokens` | `256` | Ensures **long enough responses** for better QA output. |\n| `temperature` | `0.7` | Adds controlled randomness to responses (lower = more deterministic). |\n| `num_beams` | `3` | **Beam search** for improved response coherence. |\n| `repetition_penalty` | `1.2` | **Prevents repetitive output** in responses. |\n\n### **üìå Why Beam Search?**\n- Beam search generates **more coherent answers** by considering **multiple candidate sequences**.\n- Helps **avoid incomplete or nonsensical responses**.\n\n---\n\n## **‚úÖ Final Thoughts**\n### **üîπ Techniques Implemented**\n‚úî **Instruction-based fine-tuning** with structured prompts.  \n‚úî **Memory-efficient training** with **gradient checkpointing & accumulation**.  \n‚úî **BF16 precision** for **better memory management**.  \n‚úî **Beam search & controlled randomness** for **high-quality responses**.  \n\nThis fine-tuning approach balances **performance, memory efficiency, and output quality**, ensuring that FLAN-T5 generates **accurate and informative answers** from contextual input. üöÄüî•\n","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\nfrom datasets import load_dataset\n\n# ‚úÖ Prevent CUDA memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\ndef fine_tune_flan_t5():\n    dataset = load_dataset(\"squad_v2\")\n\n    # ‚úÖ Remove examples with empty answers\n    def filter_empty_answers(example):\n        return bool(example[\"answers\"][\"text\"])  \n\n    dataset[\"train\"] = dataset[\"train\"].filter(filter_empty_answers)\n    dataset[\"validation\"] = dataset[\"validation\"].filter(filter_empty_answers)\n\n    print(f\"Train set size after filtering: {len(dataset['train'])}\")\n    print(f\"Validation set size after filtering: {len(dataset['validation'])}\")\n\n    # ‚úÖ Use a subset for memory efficiency\n    dataset[\"train\"] = dataset[\"train\"].shuffle(seed=42).select(range(2000))  \n    dataset[\"validation\"] = dataset[\"validation\"].shuffle(seed=42).select(range(500))\n\n    print(\"\\nExample data after filtering:\", dataset[\"train\"][0])\n\n    test_inputs = [\n        (dataset[\"train\"][0][\"context\"], dataset[\"train\"][0][\"question\"]),\n        (dataset[\"train\"][1][\"context\"], dataset[\"train\"][1][\"question\"])\n    ]\n\n    # ‚úÖ Use FLAN-T5 Large (Memory Optimized)\n    model_name = \"google/flan-t5-small\"  \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",  # ‚úÖ Automatically selects GPU\n        torch_dtype=torch.bfloat16,  # ‚úÖ Use BF16 instead of FP16 (Better for T5 models)\n    )\n\n    tokenizer.pad_token = tokenizer.eos_token  # ‚úÖ Ensure padding token is set\n\n    def preprocess_function(examples):\n        \"\"\" ‚úÖ Format input as structured prompt for FLAN-T5 \"\"\"\n        inputs = [\n            f\"Answer the question based on the provided context:\\n\\nContext: {c}\\n\\nQuestion: {q}\\n\\nAnswer:\"\n            for c, q in zip(examples[\"context\"], examples[\"question\"])\n        ]\n\n        # ‚úÖ Ensure answers are non-empty, otherwise use \"No Answer\"\n        outputs = [a[\"text\"][0] if len(a[\"text\"]) > 0 else \"No Answer\" for a in examples[\"answers\"]]\n\n        tokenized_inputs = tokenizer(inputs, padding=\"max_length\", truncation=True, max_length=512)\n        tokenized_outputs = tokenizer(outputs, padding=\"max_length\", truncation=True, max_length=128)\n\n        tokenized_inputs[\"labels\"] = tokenized_outputs[\"input_ids\"]\n\n        return tokenized_inputs\n\n    # ‚úÖ Apply preprocessing\n    tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n\n    # ‚úÖ Memory-Optimized Training Arguments\n    training_args = TrainingArguments(\n        output_dir=\"./flan_t5_results\",\n        evaluation_strategy=\"epoch\",\n        save_strategy=\"no\",  \n        learning_rate=3e-5,\n        per_device_train_batch_size=2,  # ‚úÖ Lower batch size to avoid OOM\n        per_device_eval_batch_size=2,\n        num_train_epochs=5,  \n        weight_decay=0.01,\n        logging_dir=\"./logs\",\n        logging_steps=10,\n        log_level=\"info\",\n        report_to=\"none\",\n        gradient_accumulation_steps=4,  # ‚úÖ Helps with batch size reduction\n        gradient_checkpointing=True,  # ‚úÖ Reduce memory by recomputing activations\n        bf16=True,  # ‚úÖ BF16 is better for T5 models\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_datasets[\"train\"],\n        eval_dataset=tokenized_datasets[\"validation\"],\n        tokenizer=tokenizer,\n    )\n\n    print(\"\\nüöÄ Starting Training...\")\n    with torch.autocast(\"cuda\", dtype=torch.bfloat16):  # ‚úÖ Enable mixed precision training\n        trainer.train()\n\n    eval_results = trainer.evaluate()\n    print(f\"\\nüî• Final Evaluation Loss: {eval_results['eval_loss']:.4f}\")\n\n    trainer.save_model(\"./fine_tuned_flan_t5\")\n    print(\"\\n‚úÖ Fine-tuned model saved!\")\n\n    return model, tokenizer, test_inputs\n\ndef test_flan_t5_model(model, tokenizer, test_inputs):\n    print(\"\\nüîç Running test queries from the training set...\\n\")\n\n    for context, query in test_inputs:\n        prompt = f\"Answer the question based on the provided context:\\n\\nContext: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n        output = model.generate(\n            **inputs, \n            max_new_tokens=256,  \n            temperature=0.7,  \n            num_beams=3,  \n            repetition_penalty=1.2,  \n            do_sample=True\n        )\n        print(f\"‚ùì Question: {query}\")\n        print(f\"üìñ Context: {context[:200]}...\")  \n        print(f\"üí° Answer: {tokenizer.decode(output[0], skip_special_tokens=True)}\\n\")\n\nif __name__ == \"__main__\":\n    model, tokenizer, test_inputs = fine_tune_flan_t5()\n    test_flan_t5_model(model, tokenizer, test_inputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-09T13:28:40.386088Z","iopub.execute_input":"2025-03-09T13:28:40.386375Z","iopub.status.idle":"2025-03-09T13:47:58.308506Z","shell.execute_reply.started":"2025-03-09T13:28:40.386352Z","shell.execute_reply":"2025-03-09T13:47:58.307805Z"}},"outputs":[],"execution_count":null}]}